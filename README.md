# Overview

This project is based on paralleling the task of Parsing the Huge [XML DBLP](https://dblp.uni-trier.de/xml/) Dataset (2GB) and Collecting the needed 
details from it. It uses the Map Reduce Model on HDFS file system. The Dataset used contains the details about the 
various publications which are made in last few decades by enumerous computer science faculty and students. This project
is about getting the details like Author names who published continously for 10 Years, Authors who published with
most number of different co-authors and Authors who published most number of paper in specific venues etc.

## Prerequisites
- Java should be installed on your system.
- Hadoop should be Installed on it.

### Alternatively 
- You can also run this jar on [HDP sandbook](https://www.cloudera.com/downloads/hortonworks-sandbox.html) which has preconfigured Hadoop in it. by downloading and Installing it
  on virtualBox or Vmware etc.

## How to Exceute the Application
1. Clone or download this repository.
2. Open the command prompt or Terminal and browse to the project directory.
3. Build the project using sbt. Use below command to compile and excecute the test cases.
	
	```
	sbt compile test
	```
	
4. Create the jar from this using the below command and note the path where jar is getting placed.
	
	```
	sbt assembly 
	```
	
5. If you are using the HDP sandbox for running the Map Reduce job then use below command to move the anyfile to
	virtual machine which is already running. Use this command to move jar and input files.
	
	``` 
	scp -P 2222 <local Path where jar is placed> root@<IP address of host machine>:<Target Path>
	```
	
	- Example
	
	```
	scp -P 2222 C:/Users/abcd/Desktop/mapReduce.jar root@192.168.116.128:~/ 
	```
	
6. Move the Input file from local file system to HDFS using this command.
	
	``` 
	hdfs dfs -put <Source Path of file in local fileSystem> <Target Path in HDFS>
	```
	
7. For exceuting the jar run this command by passing the proper arguments. 
	- Note Output directory should not be present before exceuting this command.
	
	``` 
	hadoop jar <Jar path> <Qualified name of main method> <Input Folder Path> <Generated Output Directory Name>
	```
	
	- Sample 
	
	``` 
	hadoop jar DBLPMapReduce-assembly-0.1.jar shabbir_bohra_hw2.MapReduceDriver Input Output
	```
	
8. Once the job is Successfull you can view the output in the passed Output folder.

## Implmentation Details
1. The bigger Input xml file is divided into the individual entity records using the [**custom XML Formatter**](https://bitbucket.org/sbohra3/shabbir_bohra_hw2/src/master/src/main/scala/Utils/XMLInputFormat.java) 
	which only pass the entities which are starting with the tags defined in the configuration file application.config.
2.	Since the parsing of such a big file is very time and memory intensive the Mapper, Combiner and the Reducer are
	designed in such a way that in **single parse** all the required details are obtained.
2. [**XMLMapper**](src/main/scala/XMLMapper.scala) takes the key and value generated by XMLRecordReader and converts the value into the Scala
	XML Element and fetches the required Information from it in the form of key value pairs.
3. **XMLMapper Class** Generates the following key Value pair from XML entity and forward it to the XMLCombiner for 
further processing.
	1. **Key:** AuthorAndItsCoAuthors#Author:<AuthorName> **Value:** <Co-AuthorName>
	2. **Key:** VenuePublicationAuthorCounts#Venue:<VenueName>#AuthorCount:<NumberOfAuthors> **Value:** <Name of Title>
	3. **Key:** AuthorAndPublishYear#Author:<AuthorName> **Value:**<Year>
	4. **Key** VenueAuthorCount#Venue:<VenueName>#Author:<AuthorName>**Value:**1
4. [**XMLCombiner**](https://bitbucket.org/sbohra3/shabbir_bohra_hw2/src/master/src/main/scala/XMLCombiner.scala) will further process the Key value pair generated by the Mapper and creates the following 
	key value pair.
	1. **Key:** AuthorAndItsCoAuthors **Value:** Count:<CoAuthor Count>#Author:<AuthorName>
	2. **Key:** VenueAuthorCount#Venue:<VenueName> **Value:** Count:<Number of Publication>#Author:<AuthorName>
	3. **Key:** AuthorPublishedYear **Value:** Author:<AuthorName>#YearEncoding:<1111_11>
	4. **Key:** VenuePublicationAuthorCounts#Venue:<VenueName> **Value:**AuthorCount:<NumberOfAuthors>#Titles:<titles separated by
	comma>
5. **XMLReducer** further operates on the key value pairs passed by the XMLCombiner and creates the required
	key value pair in such a way that required output is produced. Following are the key value pair generated by 
	the XMLReducer.
	1. **Key:** numberOfTopAuthorsForCoAuthorCount **Value:** [<Author1>,<Author2>,<Author3>.........]
	2. **Key:** numberOfTopAuthorsForCoAuthorCount **Value:** [<Author1>,<Author2>,<Author3>.........]
	3. **Key:** TopPublishAuthors#Venue:<VenueName> **Value:** Authors:[<author1>,<author2>,........]
	4. **Key:** AuthorsWhoPublishedContinually **Value:** Authors: [<Author1>,<Author2>,<Author3>.....]
	5. **Key:** MaxAuthorPublicationTitle#Venue:<VenueName> **Value:** Titles: [<title1>,<title2>,<title3>,........]
	6. **Key:** LeastAuthorPublicationTitle#Venue:<VenueName> **Value:** Titles: [<title1>,<title2>,<title3>,........]
6. Used Object(Singleton) class to load the Configuration details.
7. Read to the configuration details section to know more about configuration paramters

### Configuraton Details

#### Following are the configuration key value pairs.

- Configuration object defines the **valid entity tags** that **XMLRecordReader** will read from Input file
	for processing. make changes here if you want to remove or add more entity.
	
```json
	xmlTags{
	 start= ["<article ",
						"<inproceedings ",
						"<proceedings ",
						"<book ",
						"<incollection ",
						"<phdthesis ",
						"<mastersthesis "]
	end = [
		  "</article>",
		  "</inproceedings>",
		  "</proceedings>",
		  "</book>",
		  "</incollection>",
		  "</phdthesis>",
		  "</mastersthesis>"]
	}
```

- Configuration which defines the quantity of information which will get extracted in the end result.
```json
statisticsParameters
{
	numberOfContinuousYears = 10
	numberTopAuthorsAtEachVenue = 10
	numberOfTopAuthorsForCoAuthorCount = 100
	numberOfButtomAuthorsForCoAuthorCount = 100
}
```

### Output
- All the generated output are kept inside the Ouptut folder in the repository.
- CSVs are kept inside CSV folder
- Raw output after running it on EMR is kept inside EMROuput Folder.

